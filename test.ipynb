{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f6c675d2-f661-4505-98a7-58b371ac808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda as cuda\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import DeepRecursiveTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9e7275ee-14da-44ad-b993-dbd878afd313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainData(Dataset):\n",
    "    def __init__(self, dataset_dir):\n",
    "        super().__init__()\n",
    "        self.img_transforms = self.build_transform()\n",
    "        self.datasets = []\n",
    "        p = Path(dataset_dir)\n",
    "        for ext in ['png', 'jpg']:\n",
    "            self.datasets.extend(p.glob(f'*.{ext}'))\n",
    "            # self.datasets.extend(map(lambda path: path.as_posix(), p.glob(f'*.{ext}')))\n",
    "\n",
    "    def build_transform(self):\n",
    "        t = []\n",
    "        t.append(transforms.ToTensor()) #convert (B, H, W, C) from [0,255] to (B, C, H, W) [0. ,1.]\n",
    "        return transforms.Compose(t)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datasets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.datasets[idx]\n",
    "        img = Image.open(path)\n",
    "        img = self.img_transforms(img)\n",
    "        return img, path.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b400d6bd-c71c-4023-a112-3968e4f9110b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "dim = 96\n",
    "patch_size = 1\n",
    "local_window_dim = patch_size * 7\n",
    "residual_depth = 3\n",
    "recursive_depth = 6\n",
    "ckp_path = './pretrained/best_model.pt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4a2a502f-716b-4935-87d3-ad00245579f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "in_path = './datasets/Test100/input/'\n",
    "out_path = './datasets/Test100/output/'\n",
    "Path(out_path).mkdir(exist_ok=True)\n",
    "\n",
    "test_data_loader = DataLoader(RainData(in_path), batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3f623588-5182-49e8-8886-f2738a31ecff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 98/98 [06:17<00:00,  3.85s/it]\n"
     ]
    }
   ],
   "source": [
    "for net_input, img_name in tqdm(test_data_loader):\n",
    "    ### pad the image to make sure H == W fits the network requirements\n",
    "    _, _, h_old, w_old = net_input.size()\n",
    "    h_original = h_old\n",
    "    w_original = w_old\n",
    "    multiplier = max(h_old // local_window_dim + 1, w_old // local_window_dim + 1)\n",
    "    h_pad = (multiplier) * local_window_dim - h_old\n",
    "    w_pad = (multiplier) * local_window_dim - w_old\n",
    "    net_input = torch.cat([net_input, torch.flip(net_input, [2])], 2)[:, :, :h_old + h_pad, :]\n",
    "    net_input = torch.cat([net_input, torch.flip(net_input, [3])], 3)[:, :, :, :w_old + w_pad]\n",
    "    \n",
    "    ## pad again if h/w or w/h ratio is bigger than 2\n",
    "    if h_pad > h_old or w_pad > w_old:\n",
    "        _, _, h_old, w_old = net_input.size()\n",
    "        multiplier = max(h_old // local_window_dim + 1, w_old // local_window_dim + 1)\n",
    "        h_pad = (multiplier) * local_window_dim - h_old\n",
    "        w_pad = (multiplier) * local_window_dim - w_old\n",
    "        net_input = torch.cat([net_input, torch.flip(net_input, [2])], 2)[:, :, :h_old + h_pad, :]\n",
    "        net_input = torch.cat([net_input, torch.flip(net_input, [3])], 3)[:, :, :, :w_old + w_pad]\n",
    "        \n",
    "    ### evaluate | load model with each image\n",
    "    _, _, new_h, new_w = net_input.size()\n",
    "    assert new_h == new_w, \"Input image should have square dimension\"\n",
    "    eval_net = DeepRecursiveTransformer(dim, (new_h, new_w), patch_size, residual_depth, recursive_depth)\n",
    "    eval_net.load_state_dict(torch.load(ckp_path)['state_dict'])\n",
    "    eval_net.to(device)\n",
    "    eval_net.eval()\n",
    "    \n",
    "    net_input = net_input.cuda()\n",
    "    with torch.no_grad():\n",
    "        net_output = eval_net(net_input)\n",
    "        \n",
    "    ### crop the output\n",
    "    net_output = net_output[:, :, :h_original, :w_original]\n",
    "    output_data = net_output.cpu().detach().numpy() # B C H W\n",
    "    output_data = np.transpose(output_data, (0, 2, 3, 1)) # B H W C\n",
    "    output_data = np.clip(output_data * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Save\n",
    "    img = Image.fromarray(output_data[0])\n",
    "    img.save(Path(out_path, img_name[0]))\n",
    "    \n",
    "    cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649ddee-111f-43ad-bc6e-7c4b4262c94c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan1",
   "language": "python",
   "name": "gan1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

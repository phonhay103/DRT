{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b96a62bc",
   "metadata": {},
   "source": [
    "# Deep Recursive Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9077649",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6f5699a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda as cuda\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from timm.models.layers import trunc_normal_\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "#other imports\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#own files import\n",
    "from transformer_block import TransformerBlock, PatchEmbed, PatchUnEmbed\n",
    "from Data.data_loader import Rain800TrainData, Rain800ValData\n",
    "from my_utils import batch_PSNR, batch_SSIM, output_to_image\n",
    "from my_utils import save_ckp, load_ckp, base_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbb221",
   "metadata": {},
   "source": [
    "### Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8a442fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "training_image_size = 56\n",
    "dtype = torch.cuda.FloatTensor\n",
    "batch_size = 5\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed_all(1234)\n",
    "epochs = 4600\n",
    "lr = 0.0001\n",
    "error_plot_freq = 20\n",
    "INT_MAX = 2147483647\n",
    "error_tolerence = 10\n",
    "patch_size = 1\n",
    "\n",
    "#paths\n",
    "base_pth = base_path()\n",
    "ckp_pth = base_pth + \"/CheckPoints/Rain100L\"\n",
    "\n",
    "#miscellaneous\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a32126",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3346514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test100 from Hi-Net Data\n"
     ]
    }
   ],
   "source": [
    "### Prepare Data for Training\n",
    "# train_dataset = Rain800TrainData(training_image_size, dataset_dir='/Rain-800/') #/Rain100L-Train/\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=False)\n",
    "\n",
    "### Prepare synthetic data for validation\n",
    "val_syn_dataset = Rain800ValData(dataset_dir = '/Test-HiNet/Test100/', syn = False)\n",
    "val_syn_dataloader = DataLoader(dataset=val_syn_dataset, batch_size=1, pin_memory=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2715f",
   "metadata": {},
   "source": [
    "### Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8eb800",
   "metadata": {},
   "source": [
    "We inherit the swin transformer block with a modification: WSA -> CONV2d for our network. Here we design the recursive network such that every transformer blocks in the same residual units share the same weigh. We simply stack these residual units recursiely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5fb51316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#patch embedding -> transformer -> patch unembedding\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, num_heads, patch_size):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.patch_size = patch_size\n",
    "        self.input_resolution = input_resolution\n",
    "        H, W = self.input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.transformer = TransformerBlock(self.dim, (H//self.patch_size, W//self.patch_size),\n",
    "                                            num_heads) #patch size is 4\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=self.transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a5ad2b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Block -> basic block (with skip connection)\n",
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, residual_depth, patch_size):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.patch_size = patch_size\n",
    "        self.residual_depth = residual_depth\n",
    "        self.input_resolution = input_resolution\n",
    "        self.block1 = BasicBlock(self.dim, self.input_resolution, 2, self.patch_size) #multi-heads: 2\n",
    "        self.block2 = BasicBlock(self.dim, self.input_resolution, 2, self.patch_size)\n",
    "        self.conv_out = nn.Conv2d(self.dim, self.dim, 3, padding = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, HW, C = x.shape\n",
    "        H, W = self.input_resolution\n",
    "        shortcut = x\n",
    "        for _ in range(self.residual_depth):\n",
    "            x = self.block1(self.block2(x))\n",
    "            x = torch.add(x, shortcut)\n",
    "        #convolution at the end of each residual block\n",
    "        x = x.transpose(1,2).view(B, C, H//self.patch_size, W//self.patch_size)\n",
    "        x = self.conv_out(x).flatten(2).transpose(1,2)#B L C\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "594801ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recursive network based on residual units\n",
    "class DeepRecursiveTransformer(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, patch_size, residual_depth, recursive_depth):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.patch_size = patch_size\n",
    "        self.recursive_depth = recursive_depth\n",
    "        self.input_resolution = input_resolution\n",
    "        self.residual_depth = residual_depth\n",
    "        self.H, self.W = self.input_resolution\n",
    "        assert self.H == self.W, \"Input hight and width should be the same\"\n",
    "        self.input_conv1 = nn.Conv2d(3, self.dim, 3, padding=1)\n",
    "        self.patch_embed = PatchEmbed(img_size=self.H, patch_size = self.patch_size,\n",
    "                                      in_chans=3, embed_dim=self.dim)\n",
    "        self.patch_unembed = PatchUnEmbed(img_size=self.H, patch_size = self.patch_size,\n",
    "                                          in_chans=self.dim, unembed_dim=3)\n",
    "        self.recursive_layers = nn.ModuleList()\n",
    "        for i in range(self.recursive_depth):\n",
    "            layer = ResidualLayer(self.dim, self.input_resolution, self.residual_depth, self.patch_size)\n",
    "            self.recursive_layers.append(layer)\n",
    "        self.output_conv1 = nn.Conv2d(self.dim, 3, 3, padding=1)\n",
    "        #use imagenet mean and std for general domain normalisation\n",
    "        self.normalise_layer = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        self.denormalise_layer = transforms.Normalize((-0.485, -0.456, -0.406), (1./0.229, 1./0.224, 1./0.225))\n",
    "        self.apply(self._init_weights)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        \n",
    "    #weight initialisation scheme\n",
    "    def _init_weights(self, l):\n",
    "        if isinstance(l, nn.Linear):\n",
    "            trunc_normal_(l.weight, std=.02)\n",
    "            if isinstance(l, nn.Linear) and l.bias is not None:\n",
    "                nn.init.constant_(l.bias, 0)\n",
    "        elif isinstance(l, nn.LayerNorm):\n",
    "            nn.init.constant_(l.bias, 0)\n",
    "            nn.init.constant_(l.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #normalise the data, input shape (B, C, H, W)\n",
    "        x = self.normalise_layer(x)\n",
    "        outer_shortcut = x\n",
    "        x = self.patch_embed(x)\n",
    "        inner_shortcut = x\n",
    "\n",
    "        for i in range(len(self.recursive_layers)):\n",
    "            x = self.recursive_layers[i](x)\n",
    "            \n",
    "        x=torch.add(x, inner_shortcut)\n",
    "        x=self.patch_unembed(x, (self.H//self.patch_size,self.W//self.patch_size))\n",
    "        x=torch.add(x, outer_shortcut)\n",
    "        x=self.denormalise_layer(x)\n",
    "        return x #output shape (B, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2278a21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Normalize-1            [-1, 3, 56, 56]               0\n",
      "            Conv2d-2           [-1, 96, 56, 56]             384\n",
      "        PatchEmbed-3             [-1, 3136, 96]               0\n",
      "         LayerNorm-4             [-1, 3136, 96]             192\n",
      "            Linear-5              [-1, 49, 288]          27,936\n",
      "           Softmax-6            [-1, 2, 49, 49]               0\n",
      "           Dropout-7            [-1, 2, 49, 49]               0\n",
      "            Linear-8               [-1, 49, 96]           9,312\n",
      "           Dropout-9               [-1, 49, 96]               0\n",
      "  WindowAttention-10               [-1, 49, 96]               0\n",
      "         Identity-11             [-1, 3136, 96]               0\n",
      "        LayerNorm-12             [-1, 3136, 96]             192\n",
      "           Linear-13             [-1, 3136, 96]           9,312\n",
      "             GELU-14             [-1, 3136, 96]               0\n",
      "          Dropout-15             [-1, 3136, 96]               0\n",
      "           Linear-16             [-1, 3136, 96]           9,312\n",
      "          Dropout-17             [-1, 3136, 96]               0\n",
      "              Mlp-18             [-1, 3136, 96]               0\n",
      "         Identity-19             [-1, 3136, 96]               0\n",
      " TransformerBlock-20             [-1, 3136, 96]               0\n",
      "       BasicBlock-21             [-1, 3136, 96]               0\n",
      "        LayerNorm-22             [-1, 3136, 96]             192\n",
      "           Linear-23              [-1, 49, 288]          27,936\n",
      "          Softmax-24            [-1, 2, 49, 49]               0\n",
      "          Dropout-25            [-1, 2, 49, 49]               0\n",
      "           Linear-26               [-1, 49, 96]           9,312\n",
      "          Dropout-27               [-1, 49, 96]               0\n",
      "  WindowAttention-28               [-1, 49, 96]               0\n",
      "         Identity-29             [-1, 3136, 96]               0\n",
      "        LayerNorm-30             [-1, 3136, 96]             192\n",
      "           Linear-31             [-1, 3136, 96]           9,312\n",
      "             GELU-32             [-1, 3136, 96]               0\n",
      "          Dropout-33             [-1, 3136, 96]               0\n",
      "           Linear-34             [-1, 3136, 96]           9,312\n",
      "          Dropout-35             [-1, 3136, 96]               0\n",
      "              Mlp-36             [-1, 3136, 96]               0\n",
      "         Identity-37             [-1, 3136, 96]               0\n",
      " TransformerBlock-38             [-1, 3136, 96]               0\n",
      "       BasicBlock-39             [-1, 3136, 96]               0\n",
      "        LayerNorm-40             [-1, 3136, 96]             192\n",
      "           Linear-41              [-1, 49, 288]          27,936\n",
      "          Softmax-42            [-1, 2, 49, 49]               0\n",
      "          Dropout-43            [-1, 2, 49, 49]               0\n",
      "           Linear-44               [-1, 49, 96]           9,312\n",
      "          Dropout-45               [-1, 49, 96]               0\n",
      "  WindowAttention-46               [-1, 49, 96]               0\n",
      "         Identity-47             [-1, 3136, 96]               0\n",
      "        LayerNorm-48             [-1, 3136, 96]             192\n",
      "           Linear-49             [-1, 3136, 96]           9,312\n",
      "             GELU-50             [-1, 3136, 96]               0\n",
      "          Dropout-51             [-1, 3136, 96]               0\n",
      "           Linear-52             [-1, 3136, 96]           9,312\n",
      "          Dropout-53             [-1, 3136, 96]               0\n",
      "              Mlp-54             [-1, 3136, 96]               0\n",
      "         Identity-55             [-1, 3136, 96]               0\n",
      " TransformerBlock-56             [-1, 3136, 96]               0\n",
      "       BasicBlock-57             [-1, 3136, 96]               0\n",
      "        LayerNorm-58             [-1, 3136, 96]             192\n",
      "           Linear-59              [-1, 49, 288]          27,936\n",
      "          Softmax-60            [-1, 2, 49, 49]               0\n",
      "          Dropout-61            [-1, 2, 49, 49]               0\n",
      "           Linear-62               [-1, 49, 96]           9,312\n",
      "          Dropout-63               [-1, 49, 96]               0\n",
      "  WindowAttention-64               [-1, 49, 96]               0\n",
      "         Identity-65             [-1, 3136, 96]               0\n",
      "        LayerNorm-66             [-1, 3136, 96]             192\n",
      "           Linear-67             [-1, 3136, 96]           9,312\n",
      "             GELU-68             [-1, 3136, 96]               0\n",
      "          Dropout-69             [-1, 3136, 96]               0\n",
      "           Linear-70             [-1, 3136, 96]           9,312\n",
      "          Dropout-71             [-1, 3136, 96]               0\n",
      "              Mlp-72             [-1, 3136, 96]               0\n",
      "         Identity-73             [-1, 3136, 96]               0\n",
      " TransformerBlock-74             [-1, 3136, 96]               0\n",
      "       BasicBlock-75             [-1, 3136, 96]               0\n",
      "        LayerNorm-76             [-1, 3136, 96]             192\n",
      "           Linear-77              [-1, 49, 288]          27,936\n",
      "          Softmax-78            [-1, 2, 49, 49]               0\n",
      "          Dropout-79            [-1, 2, 49, 49]               0\n",
      "           Linear-80               [-1, 49, 96]           9,312\n",
      "          Dropout-81               [-1, 49, 96]               0\n",
      "  WindowAttention-82               [-1, 49, 96]               0\n",
      "         Identity-83             [-1, 3136, 96]               0\n",
      "        LayerNorm-84             [-1, 3136, 96]             192\n",
      "           Linear-85             [-1, 3136, 96]           9,312\n",
      "             GELU-86             [-1, 3136, 96]               0\n",
      "          Dropout-87             [-1, 3136, 96]               0\n",
      "           Linear-88             [-1, 3136, 96]           9,312\n",
      "          Dropout-89             [-1, 3136, 96]               0\n",
      "              Mlp-90             [-1, 3136, 96]               0\n",
      "         Identity-91             [-1, 3136, 96]               0\n",
      " TransformerBlock-92             [-1, 3136, 96]               0\n",
      "       BasicBlock-93             [-1, 3136, 96]               0\n",
      "        LayerNorm-94             [-1, 3136, 96]             192\n",
      "           Linear-95              [-1, 49, 288]          27,936\n",
      "          Softmax-96            [-1, 2, 49, 49]               0\n",
      "          Dropout-97            [-1, 2, 49, 49]               0\n",
      "           Linear-98               [-1, 49, 96]           9,312\n",
      "          Dropout-99               [-1, 49, 96]               0\n",
      " WindowAttention-100               [-1, 49, 96]               0\n",
      "        Identity-101             [-1, 3136, 96]               0\n",
      "       LayerNorm-102             [-1, 3136, 96]             192\n",
      "          Linear-103             [-1, 3136, 96]           9,312\n",
      "            GELU-104             [-1, 3136, 96]               0\n",
      "         Dropout-105             [-1, 3136, 96]               0\n",
      "          Linear-106             [-1, 3136, 96]           9,312\n",
      "         Dropout-107             [-1, 3136, 96]               0\n",
      "             Mlp-108             [-1, 3136, 96]               0\n",
      "        Identity-109             [-1, 3136, 96]               0\n",
      "TransformerBlock-110             [-1, 3136, 96]               0\n",
      "      BasicBlock-111             [-1, 3136, 96]               0\n",
      "          Conv2d-112           [-1, 96, 56, 56]          83,040\n",
      "   ResidualLayer-113             [-1, 3136, 96]               0\n",
      "       LayerNorm-114             [-1, 3136, 96]             192\n",
      "          Linear-115              [-1, 49, 288]          27,936\n",
      "         Softmax-116            [-1, 2, 49, 49]               0\n",
      "         Dropout-117            [-1, 2, 49, 49]               0\n",
      "          Linear-118               [-1, 49, 96]           9,312\n",
      "         Dropout-119               [-1, 49, 96]               0\n",
      " WindowAttention-120               [-1, 49, 96]               0\n",
      "        Identity-121             [-1, 3136, 96]               0\n",
      "       LayerNorm-122             [-1, 3136, 96]             192\n",
      "          Linear-123             [-1, 3136, 96]           9,312\n",
      "            GELU-124             [-1, 3136, 96]               0\n",
      "         Dropout-125             [-1, 3136, 96]               0\n",
      "          Linear-126             [-1, 3136, 96]           9,312\n",
      "         Dropout-127             [-1, 3136, 96]               0\n",
      "             Mlp-128             [-1, 3136, 96]               0\n",
      "        Identity-129             [-1, 3136, 96]               0\n",
      "TransformerBlock-130             [-1, 3136, 96]               0\n",
      "      BasicBlock-131             [-1, 3136, 96]               0\n",
      "       LayerNorm-132             [-1, 3136, 96]             192\n",
      "          Linear-133              [-1, 49, 288]          27,936\n",
      "         Softmax-134            [-1, 2, 49, 49]               0\n",
      "         Dropout-135            [-1, 2, 49, 49]               0\n",
      "          Linear-136               [-1, 49, 96]           9,312\n",
      "         Dropout-137               [-1, 49, 96]               0\n",
      " WindowAttention-138               [-1, 49, 96]               0\n",
      "        Identity-139             [-1, 3136, 96]               0\n",
      "       LayerNorm-140             [-1, 3136, 96]             192\n",
      "          Linear-141             [-1, 3136, 96]           9,312\n",
      "            GELU-142             [-1, 3136, 96]               0\n",
      "         Dropout-143             [-1, 3136, 96]               0\n",
      "          Linear-144             [-1, 3136, 96]           9,312\n",
      "         Dropout-145             [-1, 3136, 96]               0\n",
      "             Mlp-146             [-1, 3136, 96]               0\n",
      "        Identity-147             [-1, 3136, 96]               0\n",
      "TransformerBlock-148             [-1, 3136, 96]               0\n",
      "      BasicBlock-149             [-1, 3136, 96]               0\n",
      "       LayerNorm-150             [-1, 3136, 96]             192\n",
      "          Linear-151              [-1, 49, 288]          27,936\n",
      "         Softmax-152            [-1, 2, 49, 49]               0\n",
      "         Dropout-153            [-1, 2, 49, 49]               0\n",
      "          Linear-154               [-1, 49, 96]           9,312\n",
      "         Dropout-155               [-1, 49, 96]               0\n",
      " WindowAttention-156               [-1, 49, 96]               0\n",
      "        Identity-157             [-1, 3136, 96]               0\n",
      "       LayerNorm-158             [-1, 3136, 96]             192\n",
      "          Linear-159             [-1, 3136, 96]           9,312\n",
      "            GELU-160             [-1, 3136, 96]               0\n",
      "         Dropout-161             [-1, 3136, 96]               0\n",
      "          Linear-162             [-1, 3136, 96]           9,312\n",
      "         Dropout-163             [-1, 3136, 96]               0\n",
      "             Mlp-164             [-1, 3136, 96]               0\n",
      "        Identity-165             [-1, 3136, 96]               0\n",
      "TransformerBlock-166             [-1, 3136, 96]               0\n",
      "      BasicBlock-167             [-1, 3136, 96]               0\n",
      "       LayerNorm-168             [-1, 3136, 96]             192\n",
      "          Linear-169              [-1, 49, 288]          27,936\n",
      "         Softmax-170            [-1, 2, 49, 49]               0\n",
      "         Dropout-171            [-1, 2, 49, 49]               0\n",
      "          Linear-172               [-1, 49, 96]           9,312\n",
      "         Dropout-173               [-1, 49, 96]               0\n",
      " WindowAttention-174               [-1, 49, 96]               0\n",
      "        Identity-175             [-1, 3136, 96]               0\n",
      "       LayerNorm-176             [-1, 3136, 96]             192\n",
      "          Linear-177             [-1, 3136, 96]           9,312\n",
      "            GELU-178             [-1, 3136, 96]               0\n",
      "         Dropout-179             [-1, 3136, 96]               0\n",
      "          Linear-180             [-1, 3136, 96]           9,312\n",
      "         Dropout-181             [-1, 3136, 96]               0\n",
      "             Mlp-182             [-1, 3136, 96]               0\n",
      "        Identity-183             [-1, 3136, 96]               0\n",
      "TransformerBlock-184             [-1, 3136, 96]               0\n",
      "      BasicBlock-185             [-1, 3136, 96]               0\n",
      "       LayerNorm-186             [-1, 3136, 96]             192\n",
      "          Linear-187              [-1, 49, 288]          27,936\n",
      "         Softmax-188            [-1, 2, 49, 49]               0\n",
      "         Dropout-189            [-1, 2, 49, 49]               0\n",
      "          Linear-190               [-1, 49, 96]           9,312\n",
      "         Dropout-191               [-1, 49, 96]               0\n",
      " WindowAttention-192               [-1, 49, 96]               0\n",
      "        Identity-193             [-1, 3136, 96]               0\n",
      "       LayerNorm-194             [-1, 3136, 96]             192\n",
      "          Linear-195             [-1, 3136, 96]           9,312\n",
      "            GELU-196             [-1, 3136, 96]               0\n",
      "         Dropout-197             [-1, 3136, 96]               0\n",
      "          Linear-198             [-1, 3136, 96]           9,312\n",
      "         Dropout-199             [-1, 3136, 96]               0\n",
      "             Mlp-200             [-1, 3136, 96]               0\n",
      "        Identity-201             [-1, 3136, 96]               0\n",
      "TransformerBlock-202             [-1, 3136, 96]               0\n",
      "      BasicBlock-203             [-1, 3136, 96]               0\n",
      "       LayerNorm-204             [-1, 3136, 96]             192\n",
      "          Linear-205              [-1, 49, 288]          27,936\n",
      "         Softmax-206            [-1, 2, 49, 49]               0\n",
      "         Dropout-207            [-1, 2, 49, 49]               0\n",
      "          Linear-208               [-1, 49, 96]           9,312\n",
      "         Dropout-209               [-1, 49, 96]               0\n",
      " WindowAttention-210               [-1, 49, 96]               0\n",
      "        Identity-211             [-1, 3136, 96]               0\n",
      "       LayerNorm-212             [-1, 3136, 96]             192\n",
      "          Linear-213             [-1, 3136, 96]           9,312\n",
      "            GELU-214             [-1, 3136, 96]               0\n",
      "         Dropout-215             [-1, 3136, 96]               0\n",
      "          Linear-216             [-1, 3136, 96]           9,312\n",
      "         Dropout-217             [-1, 3136, 96]               0\n",
      "             Mlp-218             [-1, 3136, 96]               0\n",
      "        Identity-219             [-1, 3136, 96]               0\n",
      "TransformerBlock-220             [-1, 3136, 96]               0\n",
      "      BasicBlock-221             [-1, 3136, 96]               0\n",
      "          Conv2d-222           [-1, 96, 56, 56]          83,040\n",
      "   ResidualLayer-223             [-1, 3136, 96]               0\n",
      "       LayerNorm-224             [-1, 3136, 96]             192\n",
      "          Linear-225              [-1, 49, 288]          27,936\n",
      "         Softmax-226            [-1, 2, 49, 49]               0\n",
      "         Dropout-227            [-1, 2, 49, 49]               0\n",
      "          Linear-228               [-1, 49, 96]           9,312\n",
      "         Dropout-229               [-1, 49, 96]               0\n",
      " WindowAttention-230               [-1, 49, 96]               0\n",
      "        Identity-231             [-1, 3136, 96]               0\n",
      "       LayerNorm-232             [-1, 3136, 96]             192\n",
      "          Linear-233             [-1, 3136, 96]           9,312\n",
      "            GELU-234             [-1, 3136, 96]               0\n",
      "         Dropout-235             [-1, 3136, 96]               0\n",
      "          Linear-236             [-1, 3136, 96]           9,312\n",
      "         Dropout-237             [-1, 3136, 96]               0\n",
      "             Mlp-238             [-1, 3136, 96]               0\n",
      "        Identity-239             [-1, 3136, 96]               0\n",
      "TransformerBlock-240             [-1, 3136, 96]               0\n",
      "      BasicBlock-241             [-1, 3136, 96]               0\n",
      "       LayerNorm-242             [-1, 3136, 96]             192\n",
      "          Linear-243              [-1, 49, 288]          27,936\n",
      "         Softmax-244            [-1, 2, 49, 49]               0\n",
      "         Dropout-245            [-1, 2, 49, 49]               0\n",
      "          Linear-246               [-1, 49, 96]           9,312\n",
      "         Dropout-247               [-1, 49, 96]               0\n",
      " WindowAttention-248               [-1, 49, 96]               0\n",
      "        Identity-249             [-1, 3136, 96]               0\n",
      "       LayerNorm-250             [-1, 3136, 96]             192\n",
      "          Linear-251             [-1, 3136, 96]           9,312\n",
      "            GELU-252             [-1, 3136, 96]               0\n",
      "         Dropout-253             [-1, 3136, 96]               0\n",
      "          Linear-254             [-1, 3136, 96]           9,312\n",
      "         Dropout-255             [-1, 3136, 96]               0\n",
      "             Mlp-256             [-1, 3136, 96]               0\n",
      "        Identity-257             [-1, 3136, 96]               0\n",
      "TransformerBlock-258             [-1, 3136, 96]               0\n",
      "      BasicBlock-259             [-1, 3136, 96]               0\n",
      "       LayerNorm-260             [-1, 3136, 96]             192\n",
      "          Linear-261              [-1, 49, 288]          27,936\n",
      "         Softmax-262            [-1, 2, 49, 49]               0\n",
      "         Dropout-263            [-1, 2, 49, 49]               0\n",
      "          Linear-264               [-1, 49, 96]           9,312\n",
      "         Dropout-265               [-1, 49, 96]               0\n",
      " WindowAttention-266               [-1, 49, 96]               0\n",
      "        Identity-267             [-1, 3136, 96]               0\n",
      "       LayerNorm-268             [-1, 3136, 96]             192\n",
      "          Linear-269             [-1, 3136, 96]           9,312\n",
      "            GELU-270             [-1, 3136, 96]               0\n",
      "         Dropout-271             [-1, 3136, 96]               0\n",
      "          Linear-272             [-1, 3136, 96]           9,312\n",
      "         Dropout-273             [-1, 3136, 96]               0\n",
      "             Mlp-274             [-1, 3136, 96]               0\n",
      "        Identity-275             [-1, 3136, 96]               0\n",
      "TransformerBlock-276             [-1, 3136, 96]               0\n",
      "      BasicBlock-277             [-1, 3136, 96]               0\n",
      "       LayerNorm-278             [-1, 3136, 96]             192\n",
      "          Linear-279              [-1, 49, 288]          27,936\n",
      "         Softmax-280            [-1, 2, 49, 49]               0\n",
      "         Dropout-281            [-1, 2, 49, 49]               0\n",
      "          Linear-282               [-1, 49, 96]           9,312\n",
      "         Dropout-283               [-1, 49, 96]               0\n",
      " WindowAttention-284               [-1, 49, 96]               0\n",
      "        Identity-285             [-1, 3136, 96]               0\n",
      "       LayerNorm-286             [-1, 3136, 96]             192\n",
      "          Linear-287             [-1, 3136, 96]           9,312\n",
      "            GELU-288             [-1, 3136, 96]               0\n",
      "         Dropout-289             [-1, 3136, 96]               0\n",
      "          Linear-290             [-1, 3136, 96]           9,312\n",
      "         Dropout-291             [-1, 3136, 96]               0\n",
      "             Mlp-292             [-1, 3136, 96]               0\n",
      "        Identity-293             [-1, 3136, 96]               0\n",
      "TransformerBlock-294             [-1, 3136, 96]               0\n",
      "      BasicBlock-295             [-1, 3136, 96]               0\n",
      "       LayerNorm-296             [-1, 3136, 96]             192\n",
      "          Linear-297              [-1, 49, 288]          27,936\n",
      "         Softmax-298            [-1, 2, 49, 49]               0\n",
      "         Dropout-299            [-1, 2, 49, 49]               0\n",
      "          Linear-300               [-1, 49, 96]           9,312\n",
      "         Dropout-301               [-1, 49, 96]               0\n",
      " WindowAttention-302               [-1, 49, 96]               0\n",
      "        Identity-303             [-1, 3136, 96]               0\n",
      "       LayerNorm-304             [-1, 3136, 96]             192\n",
      "          Linear-305             [-1, 3136, 96]           9,312\n",
      "            GELU-306             [-1, 3136, 96]               0\n",
      "         Dropout-307             [-1, 3136, 96]               0\n",
      "          Linear-308             [-1, 3136, 96]           9,312\n",
      "         Dropout-309             [-1, 3136, 96]               0\n",
      "             Mlp-310             [-1, 3136, 96]               0\n",
      "        Identity-311             [-1, 3136, 96]               0\n",
      "TransformerBlock-312             [-1, 3136, 96]               0\n",
      "      BasicBlock-313             [-1, 3136, 96]               0\n",
      "       LayerNorm-314             [-1, 3136, 96]             192\n",
      "          Linear-315              [-1, 49, 288]          27,936\n",
      "         Softmax-316            [-1, 2, 49, 49]               0\n",
      "         Dropout-317            [-1, 2, 49, 49]               0\n",
      "          Linear-318               [-1, 49, 96]           9,312\n",
      "         Dropout-319               [-1, 49, 96]               0\n",
      " WindowAttention-320               [-1, 49, 96]               0\n",
      "        Identity-321             [-1, 3136, 96]               0\n",
      "       LayerNorm-322             [-1, 3136, 96]             192\n",
      "          Linear-323             [-1, 3136, 96]           9,312\n",
      "            GELU-324             [-1, 3136, 96]               0\n",
      "         Dropout-325             [-1, 3136, 96]               0\n",
      "          Linear-326             [-1, 3136, 96]           9,312\n",
      "         Dropout-327             [-1, 3136, 96]               0\n",
      "             Mlp-328             [-1, 3136, 96]               0\n",
      "        Identity-329             [-1, 3136, 96]               0\n",
      "TransformerBlock-330             [-1, 3136, 96]               0\n",
      "      BasicBlock-331             [-1, 3136, 96]               0\n",
      "          Conv2d-332           [-1, 96, 56, 56]          83,040\n",
      "   ResidualLayer-333             [-1, 3136, 96]               0\n",
      "       LayerNorm-334             [-1, 3136, 96]             192\n",
      "          Linear-335              [-1, 49, 288]          27,936\n",
      "         Softmax-336            [-1, 2, 49, 49]               0\n",
      "         Dropout-337            [-1, 2, 49, 49]               0\n",
      "          Linear-338               [-1, 49, 96]           9,312\n",
      "         Dropout-339               [-1, 49, 96]               0\n",
      " WindowAttention-340               [-1, 49, 96]               0\n",
      "        Identity-341             [-1, 3136, 96]               0\n",
      "       LayerNorm-342             [-1, 3136, 96]             192\n",
      "          Linear-343             [-1, 3136, 96]           9,312\n",
      "            GELU-344             [-1, 3136, 96]               0\n",
      "         Dropout-345             [-1, 3136, 96]               0\n",
      "          Linear-346             [-1, 3136, 96]           9,312\n",
      "         Dropout-347             [-1, 3136, 96]               0\n",
      "             Mlp-348             [-1, 3136, 96]               0\n",
      "        Identity-349             [-1, 3136, 96]               0\n",
      "TransformerBlock-350             [-1, 3136, 96]               0\n",
      "      BasicBlock-351             [-1, 3136, 96]               0\n",
      "       LayerNorm-352             [-1, 3136, 96]             192\n",
      "          Linear-353              [-1, 49, 288]          27,936\n",
      "         Softmax-354            [-1, 2, 49, 49]               0\n",
      "         Dropout-355            [-1, 2, 49, 49]               0\n",
      "          Linear-356               [-1, 49, 96]           9,312\n",
      "         Dropout-357               [-1, 49, 96]               0\n",
      " WindowAttention-358               [-1, 49, 96]               0\n",
      "        Identity-359             [-1, 3136, 96]               0\n",
      "       LayerNorm-360             [-1, 3136, 96]             192\n",
      "          Linear-361             [-1, 3136, 96]           9,312\n",
      "            GELU-362             [-1, 3136, 96]               0\n",
      "         Dropout-363             [-1, 3136, 96]               0\n",
      "          Linear-364             [-1, 3136, 96]           9,312\n",
      "         Dropout-365             [-1, 3136, 96]               0\n",
      "             Mlp-366             [-1, 3136, 96]               0\n",
      "        Identity-367             [-1, 3136, 96]               0\n",
      "TransformerBlock-368             [-1, 3136, 96]               0\n",
      "      BasicBlock-369             [-1, 3136, 96]               0\n",
      "       LayerNorm-370             [-1, 3136, 96]             192\n",
      "          Linear-371              [-1, 49, 288]          27,936\n",
      "         Softmax-372            [-1, 2, 49, 49]               0\n",
      "         Dropout-373            [-1, 2, 49, 49]               0\n",
      "          Linear-374               [-1, 49, 96]           9,312\n",
      "         Dropout-375               [-1, 49, 96]               0\n",
      " WindowAttention-376               [-1, 49, 96]               0\n",
      "        Identity-377             [-1, 3136, 96]               0\n",
      "       LayerNorm-378             [-1, 3136, 96]             192\n",
      "          Linear-379             [-1, 3136, 96]           9,312\n",
      "            GELU-380             [-1, 3136, 96]               0\n",
      "         Dropout-381             [-1, 3136, 96]               0\n",
      "          Linear-382             [-1, 3136, 96]           9,312\n",
      "         Dropout-383             [-1, 3136, 96]               0\n",
      "             Mlp-384             [-1, 3136, 96]               0\n",
      "        Identity-385             [-1, 3136, 96]               0\n",
      "TransformerBlock-386             [-1, 3136, 96]               0\n",
      "      BasicBlock-387             [-1, 3136, 96]               0\n",
      "       LayerNorm-388             [-1, 3136, 96]             192\n",
      "          Linear-389              [-1, 49, 288]          27,936\n",
      "         Softmax-390            [-1, 2, 49, 49]               0\n",
      "         Dropout-391            [-1, 2, 49, 49]               0\n",
      "          Linear-392               [-1, 49, 96]           9,312\n",
      "         Dropout-393               [-1, 49, 96]               0\n",
      " WindowAttention-394               [-1, 49, 96]               0\n",
      "        Identity-395             [-1, 3136, 96]               0\n",
      "       LayerNorm-396             [-1, 3136, 96]             192\n",
      "          Linear-397             [-1, 3136, 96]           9,312\n",
      "            GELU-398             [-1, 3136, 96]               0\n",
      "         Dropout-399             [-1, 3136, 96]               0\n",
      "          Linear-400             [-1, 3136, 96]           9,312\n",
      "         Dropout-401             [-1, 3136, 96]               0\n",
      "             Mlp-402             [-1, 3136, 96]               0\n",
      "        Identity-403             [-1, 3136, 96]               0\n",
      "TransformerBlock-404             [-1, 3136, 96]               0\n",
      "      BasicBlock-405             [-1, 3136, 96]               0\n",
      "       LayerNorm-406             [-1, 3136, 96]             192\n",
      "          Linear-407              [-1, 49, 288]          27,936\n",
      "         Softmax-408            [-1, 2, 49, 49]               0\n",
      "         Dropout-409            [-1, 2, 49, 49]               0\n",
      "          Linear-410               [-1, 49, 96]           9,312\n",
      "         Dropout-411               [-1, 49, 96]               0\n",
      " WindowAttention-412               [-1, 49, 96]               0\n",
      "        Identity-413             [-1, 3136, 96]               0\n",
      "       LayerNorm-414             [-1, 3136, 96]             192\n",
      "          Linear-415             [-1, 3136, 96]           9,312\n",
      "            GELU-416             [-1, 3136, 96]               0\n",
      "         Dropout-417             [-1, 3136, 96]               0\n",
      "          Linear-418             [-1, 3136, 96]           9,312\n",
      "         Dropout-419             [-1, 3136, 96]               0\n",
      "             Mlp-420             [-1, 3136, 96]               0\n",
      "        Identity-421             [-1, 3136, 96]               0\n",
      "TransformerBlock-422             [-1, 3136, 96]               0\n",
      "      BasicBlock-423             [-1, 3136, 96]               0\n",
      "       LayerNorm-424             [-1, 3136, 96]             192\n",
      "          Linear-425              [-1, 49, 288]          27,936\n",
      "         Softmax-426            [-1, 2, 49, 49]               0\n",
      "         Dropout-427            [-1, 2, 49, 49]               0\n",
      "          Linear-428               [-1, 49, 96]           9,312\n",
      "         Dropout-429               [-1, 49, 96]               0\n",
      " WindowAttention-430               [-1, 49, 96]               0\n",
      "        Identity-431             [-1, 3136, 96]               0\n",
      "       LayerNorm-432             [-1, 3136, 96]             192\n",
      "          Linear-433             [-1, 3136, 96]           9,312\n",
      "            GELU-434             [-1, 3136, 96]               0\n",
      "         Dropout-435             [-1, 3136, 96]               0\n",
      "          Linear-436             [-1, 3136, 96]           9,312\n",
      "         Dropout-437             [-1, 3136, 96]               0\n",
      "             Mlp-438             [-1, 3136, 96]               0\n",
      "        Identity-439             [-1, 3136, 96]               0\n",
      "TransformerBlock-440             [-1, 3136, 96]               0\n",
      "      BasicBlock-441             [-1, 3136, 96]               0\n",
      "          Conv2d-442           [-1, 96, 56, 56]          83,040\n",
      "   ResidualLayer-443             [-1, 3136, 96]               0\n",
      "       LayerNorm-444             [-1, 3136, 96]             192\n",
      "          Linear-445              [-1, 49, 288]          27,936\n",
      "         Softmax-446            [-1, 2, 49, 49]               0\n",
      "         Dropout-447            [-1, 2, 49, 49]               0\n",
      "          Linear-448               [-1, 49, 96]           9,312\n",
      "         Dropout-449               [-1, 49, 96]               0\n",
      " WindowAttention-450               [-1, 49, 96]               0\n",
      "        Identity-451             [-1, 3136, 96]               0\n",
      "       LayerNorm-452             [-1, 3136, 96]             192\n",
      "          Linear-453             [-1, 3136, 96]           9,312\n",
      "            GELU-454             [-1, 3136, 96]               0\n",
      "         Dropout-455             [-1, 3136, 96]               0\n",
      "          Linear-456             [-1, 3136, 96]           9,312\n",
      "         Dropout-457             [-1, 3136, 96]               0\n",
      "             Mlp-458             [-1, 3136, 96]               0\n",
      "        Identity-459             [-1, 3136, 96]               0\n",
      "TransformerBlock-460             [-1, 3136, 96]               0\n",
      "      BasicBlock-461             [-1, 3136, 96]               0\n",
      "       LayerNorm-462             [-1, 3136, 96]             192\n",
      "          Linear-463              [-1, 49, 288]          27,936\n",
      "         Softmax-464            [-1, 2, 49, 49]               0\n",
      "         Dropout-465            [-1, 2, 49, 49]               0\n",
      "          Linear-466               [-1, 49, 96]           9,312\n",
      "         Dropout-467               [-1, 49, 96]               0\n",
      " WindowAttention-468               [-1, 49, 96]               0\n",
      "        Identity-469             [-1, 3136, 96]               0\n",
      "       LayerNorm-470             [-1, 3136, 96]             192\n",
      "          Linear-471             [-1, 3136, 96]           9,312\n",
      "            GELU-472             [-1, 3136, 96]               0\n",
      "         Dropout-473             [-1, 3136, 96]               0\n",
      "          Linear-474             [-1, 3136, 96]           9,312\n",
      "         Dropout-475             [-1, 3136, 96]               0\n",
      "             Mlp-476             [-1, 3136, 96]               0\n",
      "        Identity-477             [-1, 3136, 96]               0\n",
      "TransformerBlock-478             [-1, 3136, 96]               0\n",
      "      BasicBlock-479             [-1, 3136, 96]               0\n",
      "       LayerNorm-480             [-1, 3136, 96]             192\n",
      "          Linear-481              [-1, 49, 288]          27,936\n",
      "         Softmax-482            [-1, 2, 49, 49]               0\n",
      "         Dropout-483            [-1, 2, 49, 49]               0\n",
      "          Linear-484               [-1, 49, 96]           9,312\n",
      "         Dropout-485               [-1, 49, 96]               0\n",
      " WindowAttention-486               [-1, 49, 96]               0\n",
      "        Identity-487             [-1, 3136, 96]               0\n",
      "       LayerNorm-488             [-1, 3136, 96]             192\n",
      "          Linear-489             [-1, 3136, 96]           9,312\n",
      "            GELU-490             [-1, 3136, 96]               0\n",
      "         Dropout-491             [-1, 3136, 96]               0\n",
      "          Linear-492             [-1, 3136, 96]           9,312\n",
      "         Dropout-493             [-1, 3136, 96]               0\n",
      "             Mlp-494             [-1, 3136, 96]               0\n",
      "        Identity-495             [-1, 3136, 96]               0\n",
      "TransformerBlock-496             [-1, 3136, 96]               0\n",
      "      BasicBlock-497             [-1, 3136, 96]               0\n",
      "       LayerNorm-498             [-1, 3136, 96]             192\n",
      "          Linear-499              [-1, 49, 288]          27,936\n",
      "         Softmax-500            [-1, 2, 49, 49]               0\n",
      "         Dropout-501            [-1, 2, 49, 49]               0\n",
      "          Linear-502               [-1, 49, 96]           9,312\n",
      "         Dropout-503               [-1, 49, 96]               0\n",
      " WindowAttention-504               [-1, 49, 96]               0\n",
      "        Identity-505             [-1, 3136, 96]               0\n",
      "       LayerNorm-506             [-1, 3136, 96]             192\n",
      "          Linear-507             [-1, 3136, 96]           9,312\n",
      "            GELU-508             [-1, 3136, 96]               0\n",
      "         Dropout-509             [-1, 3136, 96]               0\n",
      "          Linear-510             [-1, 3136, 96]           9,312\n",
      "         Dropout-511             [-1, 3136, 96]               0\n",
      "             Mlp-512             [-1, 3136, 96]               0\n",
      "        Identity-513             [-1, 3136, 96]               0\n",
      "TransformerBlock-514             [-1, 3136, 96]               0\n",
      "      BasicBlock-515             [-1, 3136, 96]               0\n",
      "       LayerNorm-516             [-1, 3136, 96]             192\n",
      "          Linear-517              [-1, 49, 288]          27,936\n",
      "         Softmax-518            [-1, 2, 49, 49]               0\n",
      "         Dropout-519            [-1, 2, 49, 49]               0\n",
      "          Linear-520               [-1, 49, 96]           9,312\n",
      "         Dropout-521               [-1, 49, 96]               0\n",
      " WindowAttention-522               [-1, 49, 96]               0\n",
      "        Identity-523             [-1, 3136, 96]               0\n",
      "       LayerNorm-524             [-1, 3136, 96]             192\n",
      "          Linear-525             [-1, 3136, 96]           9,312\n",
      "            GELU-526             [-1, 3136, 96]               0\n",
      "         Dropout-527             [-1, 3136, 96]               0\n",
      "          Linear-528             [-1, 3136, 96]           9,312\n",
      "         Dropout-529             [-1, 3136, 96]               0\n",
      "             Mlp-530             [-1, 3136, 96]               0\n",
      "        Identity-531             [-1, 3136, 96]               0\n",
      "TransformerBlock-532             [-1, 3136, 96]               0\n",
      "      BasicBlock-533             [-1, 3136, 96]               0\n",
      "       LayerNorm-534             [-1, 3136, 96]             192\n",
      "          Linear-535              [-1, 49, 288]          27,936\n",
      "         Softmax-536            [-1, 2, 49, 49]               0\n",
      "         Dropout-537            [-1, 2, 49, 49]               0\n",
      "          Linear-538               [-1, 49, 96]           9,312\n",
      "         Dropout-539               [-1, 49, 96]               0\n",
      " WindowAttention-540               [-1, 49, 96]               0\n",
      "        Identity-541             [-1, 3136, 96]               0\n",
      "       LayerNorm-542             [-1, 3136, 96]             192\n",
      "          Linear-543             [-1, 3136, 96]           9,312\n",
      "            GELU-544             [-1, 3136, 96]               0\n",
      "         Dropout-545             [-1, 3136, 96]               0\n",
      "          Linear-546             [-1, 3136, 96]           9,312\n",
      "         Dropout-547             [-1, 3136, 96]               0\n",
      "             Mlp-548             [-1, 3136, 96]               0\n",
      "        Identity-549             [-1, 3136, 96]               0\n",
      "TransformerBlock-550             [-1, 3136, 96]               0\n",
      "      BasicBlock-551             [-1, 3136, 96]               0\n",
      "          Conv2d-552           [-1, 96, 56, 56]          83,040\n",
      "   ResidualLayer-553             [-1, 3136, 96]               0\n",
      "       LayerNorm-554             [-1, 3136, 96]             192\n",
      "          Linear-555              [-1, 49, 288]          27,936\n",
      "         Softmax-556            [-1, 2, 49, 49]               0\n",
      "         Dropout-557            [-1, 2, 49, 49]               0\n",
      "          Linear-558               [-1, 49, 96]           9,312\n",
      "         Dropout-559               [-1, 49, 96]               0\n",
      " WindowAttention-560               [-1, 49, 96]               0\n",
      "        Identity-561             [-1, 3136, 96]               0\n",
      "       LayerNorm-562             [-1, 3136, 96]             192\n",
      "          Linear-563             [-1, 3136, 96]           9,312\n",
      "            GELU-564             [-1, 3136, 96]               0\n",
      "         Dropout-565             [-1, 3136, 96]               0\n",
      "          Linear-566             [-1, 3136, 96]           9,312\n",
      "         Dropout-567             [-1, 3136, 96]               0\n",
      "             Mlp-568             [-1, 3136, 96]               0\n",
      "        Identity-569             [-1, 3136, 96]               0\n",
      "TransformerBlock-570             [-1, 3136, 96]               0\n",
      "      BasicBlock-571             [-1, 3136, 96]               0\n",
      "       LayerNorm-572             [-1, 3136, 96]             192\n",
      "          Linear-573              [-1, 49, 288]          27,936\n",
      "         Softmax-574            [-1, 2, 49, 49]               0\n",
      "         Dropout-575            [-1, 2, 49, 49]               0\n",
      "          Linear-576               [-1, 49, 96]           9,312\n",
      "         Dropout-577               [-1, 49, 96]               0\n",
      " WindowAttention-578               [-1, 49, 96]               0\n",
      "        Identity-579             [-1, 3136, 96]               0\n",
      "       LayerNorm-580             [-1, 3136, 96]             192\n",
      "          Linear-581             [-1, 3136, 96]           9,312\n",
      "            GELU-582             [-1, 3136, 96]               0\n",
      "         Dropout-583             [-1, 3136, 96]               0\n",
      "          Linear-584             [-1, 3136, 96]           9,312\n",
      "         Dropout-585             [-1, 3136, 96]               0\n",
      "             Mlp-586             [-1, 3136, 96]               0\n",
      "        Identity-587             [-1, 3136, 96]               0\n",
      "TransformerBlock-588             [-1, 3136, 96]               0\n",
      "      BasicBlock-589             [-1, 3136, 96]               0\n",
      "       LayerNorm-590             [-1, 3136, 96]             192\n",
      "          Linear-591              [-1, 49, 288]          27,936\n",
      "         Softmax-592            [-1, 2, 49, 49]               0\n",
      "         Dropout-593            [-1, 2, 49, 49]               0\n",
      "          Linear-594               [-1, 49, 96]           9,312\n",
      "         Dropout-595               [-1, 49, 96]               0\n",
      " WindowAttention-596               [-1, 49, 96]               0\n",
      "        Identity-597             [-1, 3136, 96]               0\n",
      "       LayerNorm-598             [-1, 3136, 96]             192\n",
      "          Linear-599             [-1, 3136, 96]           9,312\n",
      "            GELU-600             [-1, 3136, 96]               0\n",
      "         Dropout-601             [-1, 3136, 96]               0\n",
      "          Linear-602             [-1, 3136, 96]           9,312\n",
      "         Dropout-603             [-1, 3136, 96]               0\n",
      "             Mlp-604             [-1, 3136, 96]               0\n",
      "        Identity-605             [-1, 3136, 96]               0\n",
      "TransformerBlock-606             [-1, 3136, 96]               0\n",
      "      BasicBlock-607             [-1, 3136, 96]               0\n",
      "       LayerNorm-608             [-1, 3136, 96]             192\n",
      "          Linear-609              [-1, 49, 288]          27,936\n",
      "         Softmax-610            [-1, 2, 49, 49]               0\n",
      "         Dropout-611            [-1, 2, 49, 49]               0\n",
      "          Linear-612               [-1, 49, 96]           9,312\n",
      "         Dropout-613               [-1, 49, 96]               0\n",
      " WindowAttention-614               [-1, 49, 96]               0\n",
      "        Identity-615             [-1, 3136, 96]               0\n",
      "       LayerNorm-616             [-1, 3136, 96]             192\n",
      "          Linear-617             [-1, 3136, 96]           9,312\n",
      "            GELU-618             [-1, 3136, 96]               0\n",
      "         Dropout-619             [-1, 3136, 96]               0\n",
      "          Linear-620             [-1, 3136, 96]           9,312\n",
      "         Dropout-621             [-1, 3136, 96]               0\n",
      "             Mlp-622             [-1, 3136, 96]               0\n",
      "        Identity-623             [-1, 3136, 96]               0\n",
      "TransformerBlock-624             [-1, 3136, 96]               0\n",
      "      BasicBlock-625             [-1, 3136, 96]               0\n",
      "       LayerNorm-626             [-1, 3136, 96]             192\n",
      "          Linear-627              [-1, 49, 288]          27,936\n",
      "         Softmax-628            [-1, 2, 49, 49]               0\n",
      "         Dropout-629            [-1, 2, 49, 49]               0\n",
      "          Linear-630               [-1, 49, 96]           9,312\n",
      "         Dropout-631               [-1, 49, 96]               0\n",
      " WindowAttention-632               [-1, 49, 96]               0\n",
      "        Identity-633             [-1, 3136, 96]               0\n",
      "       LayerNorm-634             [-1, 3136, 96]             192\n",
      "          Linear-635             [-1, 3136, 96]           9,312\n",
      "            GELU-636             [-1, 3136, 96]               0\n",
      "         Dropout-637             [-1, 3136, 96]               0\n",
      "          Linear-638             [-1, 3136, 96]           9,312\n",
      "         Dropout-639             [-1, 3136, 96]               0\n",
      "             Mlp-640             [-1, 3136, 96]               0\n",
      "        Identity-641             [-1, 3136, 96]               0\n",
      "TransformerBlock-642             [-1, 3136, 96]               0\n",
      "      BasicBlock-643             [-1, 3136, 96]               0\n",
      "       LayerNorm-644             [-1, 3136, 96]             192\n",
      "          Linear-645              [-1, 49, 288]          27,936\n",
      "         Softmax-646            [-1, 2, 49, 49]               0\n",
      "         Dropout-647            [-1, 2, 49, 49]               0\n",
      "          Linear-648               [-1, 49, 96]           9,312\n",
      "         Dropout-649               [-1, 49, 96]               0\n",
      " WindowAttention-650               [-1, 49, 96]               0\n",
      "        Identity-651             [-1, 3136, 96]               0\n",
      "       LayerNorm-652             [-1, 3136, 96]             192\n",
      "          Linear-653             [-1, 3136, 96]           9,312\n",
      "            GELU-654             [-1, 3136, 96]               0\n",
      "         Dropout-655             [-1, 3136, 96]               0\n",
      "          Linear-656             [-1, 3136, 96]           9,312\n",
      "         Dropout-657             [-1, 3136, 96]               0\n",
      "             Mlp-658             [-1, 3136, 96]               0\n",
      "        Identity-659             [-1, 3136, 96]               0\n",
      "TransformerBlock-660             [-1, 3136, 96]               0\n",
      "      BasicBlock-661             [-1, 3136, 96]               0\n",
      "          Conv2d-662           [-1, 96, 56, 56]          83,040\n",
      "   ResidualLayer-663             [-1, 3136, 96]               0\n",
      "        Upsample-664           [-1, 96, 56, 56]               0\n",
      "          Conv2d-665            [-1, 3, 56, 56]           2,595\n",
      "    PatchUnEmbed-666            [-1, 3, 56, 56]               0\n",
      "       Normalize-667            [-1, 3, 56, 56]               0\n",
      "================================================================\n",
      "Total params: 2,526,435\n",
      "Trainable params: 2,526,435\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 1037.38\n",
      "Params size (MB): 9.64\n",
      "Estimated Total Size (MB): 1047.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = DeepRecursiveTransformer(96, (training_image_size, training_image_size), patch_size, 3,6)\n",
    "summary(net.cuda(), (3, training_image_size, training_image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55818e06",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5f74a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().type(dtype)\n",
    "optimiser = optim.Adam(net.parameters(), lr=lr)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a4edf321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph network error\n",
    "def graph_error(error_list, name):\n",
    "    if name[-4:] != \".png\":\n",
    "        if name != \"\":\n",
    "            raise Exception(\"Suffix of file type is needed\")\n",
    "    save_dir = \"Losses/\" + name\n",
    "    x = np.arange(len(error_list))\n",
    "    y = np.asarray(error_list)\n",
    "    plt.plot(x, y)\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xlabel(\"Epoches\")\n",
    "    if name != \"\":\n",
    "        plt.savefig(save_dir)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b41df5",
   "metadata": {},
   "source": [
    "### Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eea27bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_training(net, optimiser, criterion, loadCkp = False, loadBest=True, new_dataset=False):\n",
    "    error_list = []\n",
    "    start_epoch = 0\n",
    "    best_model_saved = False\n",
    "    ckp_saved = False\n",
    "    previous_batch_error = INT_MAX #initialise to a large value\n",
    "    best_error = INT_MAX\n",
    "    ###load checkpoint if required\n",
    "    if loadCkp and loadBest:\n",
    "        best_model_saved = True\n",
    "        ckp_saved = True\n",
    "        #when training on a new dataset for the first time, we only load the network itself\n",
    "        if new_dataset:\n",
    "            net, _, _, _, _ = load_ckp(ckp_pth+\"/best_model.pt\", net, optimiser)\n",
    "            print(\"Finished loading the best model, ignored the training history\")\n",
    "        else:\n",
    "            net, optimiser, start_epoch, error_list, best_error = load_ckp(ckp_pth+\"/best_model.pt\", net, optimiser)\n",
    "            print(\"Finished loading the best model\")\n",
    "            previous_batch_error = best_error\n",
    "    elif loadCkp and not loadBest:\n",
    "        ckp_saved = True\n",
    "        if new_dataset:\n",
    "            net, _, _, _, _ = load_ckp(ckp_pth+\"/checkpoint.pt\", net, optimiser)\n",
    "            print(\"Finished loading the checkpoint, ignored the training history\")\n",
    "        else:\n",
    "            net, optimiser, start_epoch, error_list, best_error = load_ckp(ckp_pth+\"/checkpoint.pt\", net, optimiser)\n",
    "            print(\"Finished loading the checkpoint\")\n",
    "            previous_batch_error = best_error\n",
    "    \n",
    "    if best_error == None:\n",
    "        best_error = INT_MAX\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        batch_error = 0\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        ### iterate through the batches\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            optimiser.zero_grad()\n",
    "            target = data[0].cuda()\n",
    "            net_input = data[1].cuda()\n",
    "            net_output = net(net_input)\n",
    "            loss = criterion(net_output, target)\n",
    "            batch_error += loss.item()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        \n",
    "        ### find one epoch training time\n",
    "        one_epoch_time = time.time() - epoch_start_time\n",
    "        print(\"One epoch time: \" + str(one_epoch_time))\n",
    "        \n",
    "        ### process the error information\n",
    "        print('[%d] loss: %.3f' %(epoch + 1, batch_error))\n",
    "        ### if error is too large, roll back, otherwise save and continue\n",
    "        if batch_error > error_tolerence*previous_batch_error and (best_model_saved or ckp_saved):\n",
    "            if ckp_saved:\n",
    "                print(\"Current error is too large, loading the last checkpoint\")\n",
    "                net, optimiser, start_epoch, error_list, best_psnr = \\\n",
    "                    load_ckp(ckp_pth+\"/checkpoint.pt\", net, optimiser)\n",
    "            elif best_model_saved:\n",
    "                print(\"Current error is too large, loading the best model\")\n",
    "                net, optimiser, start_epoch, error_list, best_psnr = \\\n",
    "                    load_ckp(ckp_pth+\"/best_model.pt\", net, optimiser)\n",
    "            else:\n",
    "                raise Exception(\"Error is too large, but no models to load\")\n",
    "        else:\n",
    "            if batch_error > error_tolerence*previous_batch_error:\n",
    "                print(\"Current error is too large, but cannot roll back\")\n",
    "            else:\n",
    "                previous_batch_error = batch_error\n",
    "                \n",
    "            error_list.append(batch_error)\n",
    "            ###save the latest model\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'optimizer': optimiser.state_dict(),\n",
    "                'error_list': error_list,\n",
    "                'best_error': best_error\n",
    "            }\n",
    "            save_ckp(checkpoint, False, ckp_pth)\n",
    "            ckp_saved = True\n",
    "            \n",
    "            ###if error is the smallest save it as the best model\n",
    "            if batch_error < best_error:\n",
    "                best_error = batch_error\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': net.state_dict(),\n",
    "                    'optimizer': optimiser.state_dict(),\n",
    "                    'error_list': error_list,\n",
    "                    'best_error': best_error\n",
    "                }\n",
    "                save_ckp(checkpoint, True, ckp_pth)\n",
    "                best_model_saved = True\n",
    "                print(\"New Minimum Error Recorded!\")\n",
    "                \n",
    "            if ((epoch+1) % error_plot_freq) == 0 or epoch == epochs-1:\n",
    "                graph_error(error_list[1:], \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94a4116",
   "metadata": {},
   "source": [
    "### Network Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "409e56cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_validation(loadBest = True, display_img = False, display_all_img = False, deleted_indices = []):\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    local_window_dim = patch_size*7 #patch_size * window_size\n",
    "    deleted_indices = []\n",
    "    if display_img:\n",
    "        if display_all_img:\n",
    "            random_plot_list = np.arange(0,100)\n",
    "            if deleted_indices != []:\n",
    "                random_plot_list = np.delete(random_plot_list, deleted_indices)\n",
    "                print(random_plot_list)\n",
    "        else:\n",
    "            random_plot_list = np.random.randint(0,100, size = 20)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    ### iterate through the validation dataset\n",
    "    for j, val_data in enumerate(val_syn_dataloader, 0):\n",
    "\n",
    "        if(j not in deleted_indices):\n",
    "            if j % 50 == 0:\n",
    "                print(f\"Evaluating the {j}th sample: \")\n",
    "\n",
    "            ### pad the image to make sure H==W fits the network requirements\n",
    "            net_input = val_data[1]\n",
    "            _, _, h_old, w_old = net_input.size()\n",
    "            h_original = h_old\n",
    "            w_original = w_old\n",
    "            multiplier = max(h_old // local_window_dim + 1, w_old // local_window_dim + 1)\n",
    "            h_pad = (multiplier) * local_window_dim - h_old\n",
    "            w_pad = (multiplier) * local_window_dim - w_old\n",
    "            net_input = torch.cat([net_input, torch.flip(net_input, [2])], 2)[:, :, :h_old + h_pad, :]\n",
    "            net_input = torch.cat([net_input, torch.flip(net_input, [3])], 3)[:, :, :, :w_old + w_pad]\n",
    "\n",
    "            ## pad again if h/w or w/h ratio is bigger than 2\n",
    "            if h_pad > h_old or w_pad > w_old:\n",
    "                _, _, h_old, w_old = net_input.size()\n",
    "                multiplier = max(h_old // local_window_dim + 1, w_old // local_window_dim + 1)\n",
    "                h_pad = (multiplier) * local_window_dim - h_old\n",
    "                w_pad = (multiplier) * local_window_dim - w_old\n",
    "                net_input = torch.cat([net_input, torch.flip(net_input, [2])], 2)[:, :, :h_old + h_pad, :]\n",
    "                net_input = torch.cat([net_input, torch.flip(net_input, [3])], 3)[:, :, :, :w_old + w_pad]\n",
    "\n",
    "            ### evaluate \n",
    "            _, _, new_h, new_w = net_input.size()\n",
    "            assert new_h == new_w, \"Input image should have square dimension\"\n",
    "            #FIXME: change the fixed parameter below into adaptive variables\n",
    "            eval_net = DeepRecursiveTransformer(96, (new_h, new_w), patch_size, 3,6)\n",
    "            if loadBest:\n",
    "                eval_net,_,_,_,_ = load_ckp(ckp_pth+\"/best_model.pt\", eval_net, optimiser)\n",
    "            else:\n",
    "                eval_net,_,_,_,_ = load_ckp(ckp_pth+\"/checkpoint.pt\", eval_net, optimiser)\n",
    "\n",
    "            net_input = net_input.cuda()\n",
    "            target = val_data[0].cuda()\n",
    "            eval_net = eval_net.to(device)\n",
    "            eval_net.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                net_output = eval_net(net_input)\n",
    "\n",
    "            ### crop the output\n",
    "            net_output = net_output[:,:,:h_original, :w_original]\n",
    "            output_data = net_output.cpu().detach().numpy() #B C H W\n",
    "            output_data = np.transpose(output_data, (0,2,3,1)) #B H W C\n",
    "            target_data = target.cpu().detach().numpy()\n",
    "            target_data = np.transpose(target_data, (0,2,3,1))\n",
    "            input_data = val_data[1].cpu().detach().numpy()\n",
    "            input_data = np.transpose(input_data, (0,2,3,1))\n",
    "            \n",
    "            #display\n",
    "            if display_img:\n",
    "                if (j in random_plot_list) or j == 0:\n",
    "                    output_to_image(target_data, output_data, input_data, save_img = True, save_index = j)\n",
    "            \n",
    "            \n",
    "            ### calculate the PSNR and SSIM\n",
    "            psnr_list.append(batch_PSNR(target_data, output_data, True).item())\n",
    "            ssim_list.append(batch_SSIM(target_data, output_data).item())\n",
    "            cuda.empty_cache()\n",
    "            \n",
    "    run_time = time.time() - start_time\n",
    "    avg_psnr = sum(psnr_list)/len(psnr_list)\n",
    "    avg_ssim = sum(ssim_list)/len(ssim_list)\n",
    "    print(f\"Average PSNR is {avg_psnr}\")\n",
    "    print(f\"Average SSIM is {avg_ssim}\")\n",
    "    print(f\"Validation time is {run_time}\")\n",
    "    return avg_psnr, avg_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e976005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_net = True, loadCkp = True, loadBest = True, new_dataset=False):\n",
    "    if train_net:\n",
    "        print(\"Training the network...\")\n",
    "        network_training(net, optimiser, criterion, loadCkp=loadCkp, loadBest=loadBest, new_dataset=new_dataset)\n",
    "    else:\n",
    "        print(\"Evaluating the network...\")\n",
    "        network_validation(display_img = False, display_all_img = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "12ec3353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the network...\n",
      "Evaluating the 0th sample: \n",
      "Evaluating the 50th sample: \n",
      "Average PSNR is 22.545935928393387\n",
      "Average SSIM is 0.7051708323828185\n",
      "Validation time is 399.29389905929565\n"
     ]
    }
   ],
   "source": [
    "run(train_net = False, loadCkp = True, loadBest = True, new_dataset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdab6d1-4a1d-4cae-97fa-8a5ae4a392b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan1",
   "language": "python",
   "name": "gan1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

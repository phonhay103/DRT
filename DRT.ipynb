{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b96a62bc",
   "metadata": {},
   "source": [
    "# Deep Recursive Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9077649",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5699a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda as cuda\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from timm.models.layers import trunc_normal_\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "import time\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "from transformer_block import TransformerBlock, PatchEmbed, PatchUnEmbed\n",
    "from data_loader import Rain800TrainData, Rain800ValData\n",
    "from my_utils import batch_PSNR, batch_SSIM, output_to_image\n",
    "from my_utils import save_ckp, load_ckp, base_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbb221",
   "metadata": {},
   "source": [
    "### Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a442fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "training_image_size = 56\n",
    "dtype = torch.cuda.FloatTensor\n",
    "batch_size = 5\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed_all(1234)\n",
    "epochs = 4600\n",
    "lr = 0.0001\n",
    "error_plot_freq = 20\n",
    "INT_MAX = 2147483647\n",
    "error_tolerence = 10\n",
    "patch_size = 1\n",
    "\n",
    "#paths\n",
    "base_pth = base_path()\n",
    "ckp_pth = base_pth + \"/pretrained\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a32126",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3346514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test1200 from Hi-Net Data\n"
     ]
    }
   ],
   "source": [
    "### Prepare Data for Training\n",
    "# train_dataset = Rain800TrainData(training_image_size, dataset_dir='/Rain-800/') #/Rain100L-Train/\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2715f",
   "metadata": {},
   "source": [
    "### Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8eb800",
   "metadata": {},
   "source": [
    "We inherit the swin transformer block with a modification: WSA -> CONV2d for our network. Here we design the recursive network such that every transformer blocks in the same residual units share the same weigh. We simply stack these residual units recursiely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb51316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#patch embedding -> transformer -> patch unembedding\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, num_heads, patch_size):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.patch_size = patch_size\n",
    "        self.input_resolution = input_resolution\n",
    "        H, W = self.input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.transformer = TransformerBlock(self.dim, (H//self.patch_size, W//self.patch_size),\n",
    "                                            num_heads) #patch size is 4\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=self.transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5ad2b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Block -> basic block (with skip connection)\n",
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, residual_depth, patch_size):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.patch_size = patch_size\n",
    "        self.residual_depth = residual_depth\n",
    "        self.input_resolution = input_resolution\n",
    "        self.block1 = BasicBlock(self.dim, self.input_resolution, 2, self.patch_size) #multi-heads: 2\n",
    "        self.block2 = BasicBlock(self.dim, self.input_resolution, 2, self.patch_size)\n",
    "        self.conv_out = nn.Conv2d(self.dim, self.dim, 3, padding = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, HW, C = x.shape\n",
    "        H, W = self.input_resolution\n",
    "        shortcut = x\n",
    "        for _ in range(self.residual_depth):\n",
    "            x = self.block1(self.block2(x))\n",
    "            x = torch.add(x, shortcut)\n",
    "        #convolution at the end of each residual block\n",
    "        x = x.transpose(1,2).view(B, C, H//self.patch_size, W//self.patch_size)\n",
    "        x = self.conv_out(x).flatten(2).transpose(1,2)#B L C\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594801ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recursive network based on residual units\n",
    "class DeepRecursiveTransformer(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, patch_size, residual_depth, recursive_depth):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.patch_size = patch_size\n",
    "        self.recursive_depth = recursive_depth\n",
    "        self.input_resolution = input_resolution\n",
    "        self.residual_depth = residual_depth\n",
    "        self.H, self.W = self.input_resolution\n",
    "        assert self.H == self.W, \"Input hight and width should be the same\"\n",
    "        self.input_conv1 = nn.Conv2d(3, self.dim, 3, padding=1)\n",
    "        self.patch_embed = PatchEmbed(img_size=self.H, patch_size = self.patch_size,\n",
    "                                      in_chans=3, embed_dim=self.dim)\n",
    "        self.patch_unembed = PatchUnEmbed(img_size=self.H, patch_size = self.patch_size,\n",
    "                                          in_chans=self.dim, unembed_dim=3)\n",
    "        self.recursive_layers = nn.ModuleList()\n",
    "        for i in range(self.recursive_depth):\n",
    "            layer = ResidualLayer(self.dim, self.input_resolution, self.residual_depth, self.patch_size)\n",
    "            self.recursive_layers.append(layer)\n",
    "        self.output_conv1 = nn.Conv2d(self.dim, 3, 3, padding=1)\n",
    "        #use imagenet mean and std for general domain normalisation\n",
    "        self.normalise_layer = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        self.denormalise_layer = transforms.Normalize((-0.485, -0.456, -0.406), (1./0.229, 1./0.224, 1./0.225))\n",
    "        self.apply(self._init_weights)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        \n",
    "    #weight initialisation scheme\n",
    "    def _init_weights(self, l):\n",
    "        if isinstance(l, nn.Linear):\n",
    "            trunc_normal_(l.weight, std=.02)\n",
    "            if isinstance(l, nn.Linear) and l.bias is not None:\n",
    "                nn.init.constant_(l.bias, 0)\n",
    "        elif isinstance(l, nn.LayerNorm):\n",
    "            nn.init.constant_(l.bias, 0)\n",
    "            nn.init.constant_(l.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #normalise the data, input shape (B, C, H, W)\n",
    "        x = self.normalise_layer(x)\n",
    "        outer_shortcut = x\n",
    "        x = self.patch_embed(x)\n",
    "        inner_shortcut = x\n",
    "\n",
    "        for i in range(len(self.recursive_layers)):\n",
    "            x = self.recursive_layers[i](x)\n",
    "            \n",
    "        x=torch.add(x, inner_shortcut)\n",
    "        x=self.patch_unembed(x, (self.H//self.patch_size,self.W//self.patch_size))\n",
    "        x=torch.add(x, outer_shortcut)\n",
    "        x=self.denormalise_layer(x)\n",
    "        return x #output shape (B, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2278a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 96\n",
    "input_shape = (training_image_size, training_image_size)\n",
    "patch_size = 1\n",
    "residual_depth = 3\n",
    "recursive_depth = 6\n",
    "net = DeepRecursiveTransformer(dim, input_shape, patch_size, residual_depth, recursive_depth)\n",
    "# summary(net.cuda(), (3, training_image_size, training_image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55818e06",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f74a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().type(dtype)\n",
    "optimiser = optim.Adam(net.parameters(), lr=lr)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4edf321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph network error\n",
    "def graph_error(error_list, name):\n",
    "    if name[-4:] != \".png\":\n",
    "        if name != \"\":\n",
    "            raise Exception(\"Suffix of file type is needed\")\n",
    "    save_dir = \"Losses/\" + name\n",
    "    x = np.arange(len(error_list))\n",
    "    y = np.asarray(error_list)\n",
    "    plt.plot(x, y)\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xlabel(\"Epoches\")\n",
    "    if name != \"\":\n",
    "        plt.savefig(save_dir)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b41df5",
   "metadata": {},
   "source": [
    "### Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eea27bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_training(net, optimiser, criterion, loadCkp = False, loadBest=True, new_dataset=False):\n",
    "    error_list = []\n",
    "    start_epoch = 0\n",
    "    best_model_saved = False\n",
    "    ckp_saved = False\n",
    "    previous_batch_error = INT_MAX #initialise to a large value\n",
    "    best_error = INT_MAX\n",
    "    ###load checkpoint if required\n",
    "    if loadCkp and loadBest:\n",
    "        best_model_saved = True\n",
    "        ckp_saved = True\n",
    "        #when training on a new dataset for the first time, we only load the network itself\n",
    "        if new_dataset:\n",
    "            net, _, _, _, _ = load_ckp(ckp_pth+\"/best_model.pt\", net, optimiser)\n",
    "            print(\"Finished loading the best model, ignored the training history\")\n",
    "        else:\n",
    "            net, optimiser, start_epoch, error_list, best_error = load_ckp(ckp_pth+\"/best_model.pt\", net, optimiser)\n",
    "            print(\"Finished loading the best model\")\n",
    "            previous_batch_error = best_error\n",
    "    elif loadCkp and not loadBest:\n",
    "        ckp_saved = True\n",
    "        if new_dataset:\n",
    "            net, _, _, _, _ = load_ckp(ckp_pth+\"/checkpoint.pt\", net, optimiser)\n",
    "            print(\"Finished loading the checkpoint, ignored the training history\")\n",
    "        else:\n",
    "            net, optimiser, start_epoch, error_list, best_error = load_ckp(ckp_pth+\"/checkpoint.pt\", net, optimiser)\n",
    "            print(\"Finished loading the checkpoint\")\n",
    "            previous_batch_error = best_error\n",
    "    \n",
    "    if best_error == None:\n",
    "        best_error = INT_MAX\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        batch_error = 0\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        ### iterate through the batches\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            optimiser.zero_grad()\n",
    "            target = data[0].cuda()\n",
    "            net_input = data[1].cuda()\n",
    "            net_output = net(net_input)\n",
    "            loss = criterion(net_output, target)\n",
    "            batch_error += loss.item()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        \n",
    "        ### find one epoch training time\n",
    "        one_epoch_time = time.time() - epoch_start_time\n",
    "        print(\"One epoch time: \" + str(one_epoch_time))\n",
    "        \n",
    "        ### process the error information\n",
    "        print('[%d] loss: %.3f' %(epoch + 1, batch_error))\n",
    "        ### if error is too large, roll back, otherwise save and continue\n",
    "        if batch_error > error_tolerence*previous_batch_error and (best_model_saved or ckp_saved):\n",
    "            if ckp_saved:\n",
    "                print(\"Current error is too large, loading the last checkpoint\")\n",
    "                net, optimiser, start_epoch, error_list, best_psnr = \\\n",
    "                    load_ckp(ckp_pth+\"/checkpoint.pt\", net, optimiser)\n",
    "            elif best_model_saved:\n",
    "                print(\"Current error is too large, loading the best model\")\n",
    "                net, optimiser, start_epoch, error_list, best_psnr = \\\n",
    "                    load_ckp(ckp_pth+\"/best_model.pt\", net, optimiser)\n",
    "            else:\n",
    "                raise Exception(\"Error is too large, but no models to load\")\n",
    "        else:\n",
    "            if batch_error > error_tolerence*previous_batch_error:\n",
    "                print(\"Current error is too large, but cannot roll back\")\n",
    "            else:\n",
    "                previous_batch_error = batch_error\n",
    "                \n",
    "            error_list.append(batch_error)\n",
    "            ###save the latest model\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'optimizer': optimiser.state_dict(),\n",
    "                'error_list': error_list,\n",
    "                'best_error': best_error\n",
    "            }\n",
    "            save_ckp(checkpoint, False, ckp_pth)\n",
    "            ckp_saved = True\n",
    "            \n",
    "            ###if error is the smallest save it as the best model\n",
    "            if batch_error < best_error:\n",
    "                best_error = batch_error\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': net.state_dict(),\n",
    "                    'optimizer': optimiser.state_dict(),\n",
    "                    'error_list': error_list,\n",
    "                    'best_error': best_error\n",
    "                }\n",
    "                save_ckp(checkpoint, True, ckp_pth)\n",
    "                best_model_saved = True\n",
    "                print(\"New Minimum Error Recorded!\")\n",
    "                \n",
    "            if ((epoch+1) % error_plot_freq) == 0 or epoch == epochs-1:\n",
    "                graph_error(error_list[1:], \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan1",
   "language": "python",
   "name": "gan1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9077649",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5699a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.cuda as cuda\n",
    "# import torch.optim as optim\n",
    "# from torchvision import datasets, transforms\n",
    "# from timm.models.layers import trunc_normal_\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchsummary import summary\n",
    "\n",
    "# import numpy as np\n",
    "# import cv2 as cv\n",
    "# from PIL import Image\n",
    "# import time\n",
    "# import warnings\n",
    "# import time\n",
    "\n",
    "from model import DeepRecursiveTransformer\n",
    "# from my_utils import batch_PSNR, batch_SSIM, output_to_image\n",
    "# from my_utils import save_ckp, load_ckp, base_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbb221",
   "metadata": {},
   "source": [
    "### Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a442fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "training_image_size = 56\n",
    "dtype = torch.cuda.FloatTensor\n",
    "batch_size = 5\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed_all(1234)\n",
    "epochs = 4600\n",
    "lr = 0.0001\n",
    "error_plot_freq = 20\n",
    "INT_MAX = 2147483647\n",
    "error_tolerence = 10\n",
    "\n",
    "#paths\n",
    "base_pth = base_path()\n",
    "ckp_pth = base_pth + \"/pretrained\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a32126",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3346514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test1200 from Hi-Net Data\n"
     ]
    }
   ],
   "source": [
    "### Prepare Data for Training\n",
    "# train_dataset = Rain800TrainData(training_image_size, dataset_dir='/Rain-800/') #/Rain100L-Train/\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2278a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 96\n",
    "input_shape = (training_image_size, training_image_size)\n",
    "patch_size = 1\n",
    "residual_depth = 3\n",
    "recursive_depth = 6\n",
    "net = DeepRecursiveTransformer(dim, input_shape, patch_size, residual_depth, recursive_depth)\n",
    "# summary(net.cuda(), (3, training_image_size, training_image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55818e06",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f74a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().type(dtype)\n",
    "optimiser = optim.Adam(net.parameters(), lr=lr)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4edf321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph network error\n",
    "def graph_error(error_list, name):\n",
    "    if name[-4:] != \".png\":\n",
    "        if name != \"\":\n",
    "            raise Exception(\"Suffix of file type is needed\")\n",
    "    save_dir = \"Losses/\" + name\n",
    "    x = np.arange(len(error_list))\n",
    "    y = np.asarray(error_list)\n",
    "    plt.plot(x, y)\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xlabel(\"Epoches\")\n",
    "    if name != \"\":\n",
    "        plt.savefig(save_dir)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b41df5",
   "metadata": {},
   "source": [
    "### Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eea27bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_training(net, optimiser, criterion, loadCkp = False, loadBest=True, new_dataset=False):\n",
    "    error_list = []\n",
    "    start_epoch = 0\n",
    "    best_model_saved = False\n",
    "    ckp_saved = False\n",
    "    previous_batch_error = INT_MAX #initialise to a large value\n",
    "    best_error = INT_MAX\n",
    "    ###load checkpoint if required\n",
    "    if loadCkp and loadBest:\n",
    "        best_model_saved = True\n",
    "        ckp_saved = True\n",
    "        #when training on a new dataset for the first time, we only load the network itself\n",
    "        if new_dataset:\n",
    "            net, _, _, _, _ = load_ckp(ckp_pth+\"/best_model.pt\", net, optimiser)\n",
    "            print(\"Finished loading the best model, ignored the training history\")\n",
    "        else:\n",
    "            net, optimiser, start_epoch, error_list, best_error = load_ckp(ckp_pth+\"/best_model.pt\", net, optimiser)\n",
    "            print(\"Finished loading the best model\")\n",
    "            previous_batch_error = best_error\n",
    "    elif loadCkp and not loadBest:\n",
    "        ckp_saved = True\n",
    "        if new_dataset:\n",
    "            net, _, _, _, _ = load_ckp(ckp_pth+\"/checkpoint.pt\", net, optimiser)\n",
    "            print(\"Finished loading the checkpoint, ignored the training history\")\n",
    "        else:\n",
    "            net, optimiser, start_epoch, error_list, best_error = load_ckp(ckp_pth+\"/checkpoint.pt\", net, optimiser)\n",
    "            print(\"Finished loading the checkpoint\")\n",
    "            previous_batch_error = best_error\n",
    "    \n",
    "    if best_error == None:\n",
    "        best_error = INT_MAX\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        batch_error = 0\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        ### iterate through the batches\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            optimiser.zero_grad()\n",
    "            target = data[0].cuda()\n",
    "            net_input = data[1].cuda()\n",
    "            net_output = net(net_input)\n",
    "            loss = criterion(net_output, target)\n",
    "            batch_error += loss.item()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        \n",
    "        ### find one epoch training time\n",
    "        one_epoch_time = time.time() - epoch_start_time\n",
    "        print(\"One epoch time: \" + str(one_epoch_time))\n",
    "        \n",
    "        ### process the error information\n",
    "        print('[%d] loss: %.3f' %(epoch + 1, batch_error))\n",
    "        ### if error is too large, roll back, otherwise save and continue\n",
    "        if batch_error > error_tolerence*previous_batch_error and (best_model_saved or ckp_saved):\n",
    "            if ckp_saved:\n",
    "                print(\"Current error is too large, loading the last checkpoint\")\n",
    "                net, optimiser, start_epoch, error_list, best_psnr = \\\n",
    "                    load_ckp(ckp_pth+\"/checkpoint.pt\", net, optimiser)\n",
    "            elif best_model_saved:\n",
    "                print(\"Current error is too large, loading the best model\")\n",
    "                net, optimiser, start_epoch, error_list, best_psnr = \\\n",
    "                    load_ckp(ckp_pth+\"/best_model.pt\", net, optimiser)\n",
    "            else:\n",
    "                raise Exception(\"Error is too large, but no models to load\")\n",
    "        else:\n",
    "            if batch_error > error_tolerence*previous_batch_error:\n",
    "                print(\"Current error is too large, but cannot roll back\")\n",
    "            else:\n",
    "                previous_batch_error = batch_error\n",
    "                \n",
    "            error_list.append(batch_error)\n",
    "            ###save the latest model\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'optimizer': optimiser.state_dict(),\n",
    "                'error_list': error_list,\n",
    "                'best_error': best_error\n",
    "            }\n",
    "            save_ckp(checkpoint, False, ckp_pth)\n",
    "            ckp_saved = True\n",
    "            \n",
    "            ###if error is the smallest save it as the best model\n",
    "            if batch_error < best_error:\n",
    "                best_error = batch_error\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': net.state_dict(),\n",
    "                    'optimizer': optimiser.state_dict(),\n",
    "                    'error_list': error_list,\n",
    "                    'best_error': best_error\n",
    "                }\n",
    "                save_ckp(checkpoint, True, ckp_pth)\n",
    "                best_model_saved = True\n",
    "                print(\"New Minimum Error Recorded!\")\n",
    "                \n",
    "            if ((epoch+1) % error_plot_freq) == 0 or epoch == epochs-1:\n",
    "                graph_error(error_list[1:], \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan1",
   "language": "python",
   "name": "gan1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
